{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "tf.random.set_seed(42)\n",
    "from nlp_model_text_preprocessing import index_the_words, text_to_sequence, pad_sequences, one_hot_encoding, index_the_char, text_to_sequence_char, char_sequence_to_text, word_sequence_to_text\n",
    "from arabic_text_normalization import text_normalization\n",
    "from deep_learning import nlp_model_word, nlp_model_char, model_compile_word, model_compile_char, model_fit, plot_word_model_change, plot_char_model_change\n",
    "from model_testing import model_testing_char, model_testing_word\n",
    "from transformers_models import load_dataset, data_collator, train_arguments, training_, save_model_tokenizer, transformer_testing,transformes_model\n",
    "from read_data import read_file\n",
    "from generate_train_label import generate_train_label_word, generate_train_label_char\n",
    "from model_check_point import check_point\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ignore Warnings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1242"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Corpus = read_file(file_path='الخيميائي.txt', text_normalization=text_normalization)\n",
    "len(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Cleaned_Corpus.txt', 'w') as f:\n",
    "    for line in Corpus:\n",
    "        f.write(line + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Early Stop Depending On Value Of Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_ = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,\n",
    "    mode = 'min',\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-Tokens Based On Word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word To Index, Index To Word, And Find Count Of All Words\n",
    "all_words, words_index, index_to_words = index_the_words(Corpus)\n",
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_word, train, labels = generate_train_label_word(Corpus, text_to_sequence, words_index, pad_sequences)\n",
    "max_length_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Label Data \n",
    "label = one_hot_encoding(labels, all_words)\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Deep Learning Models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-LSTM\n",
    "arabic_lstm_check_point_1 = check_point('Arabic_Lstm_1')\n",
    "lstm_model = tf.keras.layers.LSTM(units=128, return_sequences=False)\n",
    "LSTM_1 = nlp_model_word(input_dim = all_words, output_dim = 100, input_length = max_length_word, unit = all_words, model = lstm_model)\n",
    "model_compile_word(model =LSTM_1, optimizer=tf.keras.optimizers.legacy.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "history = model_fit(model=LSTM_1, Data=train, Label=label, epochs=150, early_stop=early_stop_, checkpoint=arabic_lstm_check_point_1 ,batch_size=32)\n",
    "plot_word_model_change(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Bidirectional LSTM\n",
    "arabic_bidirectional_lstm_check_point_1 = check_point('Arabic_Bidirectional_1')\n",
    "bidirectional_lstm_model = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units= 128, return_sequences=False))\n",
    "Bidirectional_LSTM_1 = nlp_model_word(input_dim = all_words, output_dim = 100, input_length = max_length_word, unit = all_words, model = bidirectional_lstm_model)\n",
    "model_compile_word(model =Bidirectional_LSTM_1, optimizer=tf.keras.optimizers.legacy.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "history = model_fit(model=Bidirectional_LSTM_1, Data=train, Label=label, epochs=150, early_stop=early_stop_, checkpoint=arabic_bidirectional_lstm_check_point_1, batch_size=64)\n",
    "plot_word_model_change(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-GRU\n",
    "arabic_gru_check_point_1 = check_point('Arabic_GRU1')\n",
    "gru_model = tf.keras.layers.GRU(units= 128, return_sequences=False)\n",
    "GRU_1 = nlp_model_word(input_dim = all_words, output_dim = 100, input_length = max_length_word, unit = all_words, model = gru_model)\n",
    "model_compile_word(model =GRU_1, optimizer=tf.keras.optimizers.legacy.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "history = model_fit(model=GRU_1, Data=train, Label=label, epochs=150, early_stop=early_stop_, checkpoint=arabic_gru_check_point_1 , batch_size=32)\n",
    "plot_word_model_change(history=history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Testing The Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-LSTM\n",
    "model_testing_word(text = 'تناول الخيميائي ', number_of_words = 50, text_normalization = text_normalization, text_to_sequence = text_to_sequence, words_index = words_index, pad_sequences = pad_sequences, checkpoint_filepath='Arabic_Lstm_1_model_checkpoint.h5', max_length = max_length_word, index_to_words = index_to_words, word_sequence_to_text = word_sequence_to_text, all_words=all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Bidirectional LSTM\n",
    "model_testing_word(text = 'رأى تاجر الزجاجيّات شروق ', number_of_words = 50, text_normalization = text_normalization, text_to_sequence = text_to_sequence, words_index = words_index, pad_sequences = pad_sequences, checkpoint_filepath='Arabic_Bidirectional_1_model_checkpoint.h5', max_length = max_length_word, index_to_words = index_to_words, word_sequence_to_text = word_sequence_to_text, all_words=all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-GRU\n",
    "model_testing_word(text = 'إن مافهمه في تلك اللحظة ', number_of_words = 50, text_normalization = text_normalization, text_to_sequence = text_to_sequence, words_index = words_index, pad_sequences = pad_sequences, checkpoint_filepath='Arabic_GRU1_model_checkpoint.h5', max_length = max_length_word, index_to_words = index_to_words, word_sequence_to_text = word_sequence_to_text, all_words=all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-Token Based On Character**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = Corpus[0:5]\n",
    "Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Char To Index, Index To Char, And Find Count Of All Char\n",
    "all_chars, chars_index, index_chars = index_the_char(Corpus)\n",
    "all_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'ء': 1,\n",
       " 'آ': 2,\n",
       " 'أ': 3,\n",
       " 'ؤ': 4,\n",
       " 'إ': 5,\n",
       " 'ئ': 6,\n",
       " 'ا': 7,\n",
       " 'ب': 8,\n",
       " 'ة': 9,\n",
       " 'ت': 10,\n",
       " 'ث': 11,\n",
       " 'ج': 12,\n",
       " 'ح': 13,\n",
       " 'خ': 14,\n",
       " 'د': 15,\n",
       " 'ذ': 16,\n",
       " 'ر': 17,\n",
       " 'ز': 18,\n",
       " 'س': 19,\n",
       " 'ش': 20,\n",
       " 'ص': 21,\n",
       " 'ض': 22,\n",
       " 'ط': 23,\n",
       " 'ظ': 24,\n",
       " 'ع': 25,\n",
       " 'غ': 26,\n",
       " 'ف': 27,\n",
       " 'ق': 28,\n",
       " 'ك': 29,\n",
       " 'ل': 30,\n",
       " 'م': 31,\n",
       " 'ن': 32,\n",
       " 'ه': 33,\n",
       " 'و': 34,\n",
       " 'ى': 35,\n",
       " 'ي': 36,\n",
       " 'ً': 37,\n",
       " 'ٌ': 38,\n",
       " 'ٍ': 39,\n",
       " 'َ': 40,\n",
       " 'ُ': 41,\n",
       " 'ِ': 42,\n",
       " 'ّ': 43,\n",
       " 'ْ': 44,\n",
       " 'UNK': 45}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "866"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find The Max Length\n",
    "max_length_char = max([len(s) for s in Corpus])\n",
    "max_length_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Train And Label Data\n",
    "Train, Label = generate_train_label_char(Corpus, max_length_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Words Into Number\n",
    "sequence_text_char_Train = text_to_sequence_char(chars_index, Train)\n",
    "sequence_text_char_Label = text_to_sequence_char(chars_index, Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding The The Input Sequence To Make All Sequence In Same Length\n",
    "Train = pad_sequences(input_sequence=sequence_text_char_Train, max_length=max_length_char, padding='post')\n",
    "# # Convert The Label Data\n",
    "Label = one_hot_encoding(sequence_text_char_Label, all_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-LSTM\n",
    "arabic_bidirectional_lstm_check_point_2 = check_point('Arabic_Bidirectional_2')\n",
    "lstm_model = tf.keras.layers.LSTM(units= 1024, return_sequences=False)\n",
    "LSTM_2 = nlp_model_char(input_dim = all_chars, output_dim = 100, unit = all_chars, model = lstm_model, input_length=max_length_char)\n",
    "model_compile_char(model =LSTM_2, optimizer=tf.keras.optimizers.legacy.Adam(), loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "history = model_fit(model=LSTM_2, Data=Train, Label=Label, epochs=150, early_stop=early_stop_,  batch_size=32, checkpoint=arabic_bidirectional_lstm_check_point_2)\n",
    "plot_char_model_change(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(128, 865), dtype=tf.int32, name=None), TensorSpec(shape=(128, 865), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = max_length_char\n",
    "examples_per_epoch = len(Corpus)//seq_length\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(sequence_text_char_Train)\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "dataset = char_dataset.map(split_input_target)\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),                         \n",
    "    tf.keras.layers.LSTM(rnn_units, \n",
    "                          return_sequences=True,\n",
    "                          stateful=True, recurrent_initializer='glorot_uniform'),                     \n",
    "    \n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM( batch_input_shape=[batch_size, None])),                                     \n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = all_chars,\n",
    "  embedding_dim=100,\n",
    "  rnn_units=128,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "7/7 [==============================] - 4s 315ms/step - loss: 3.7692\n",
      "Epoch 2/300\n",
      "7/7 [==============================] - 2s 312ms/step - loss: 3.3304\n",
      "Epoch 3/300\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 3.1083\n",
      "Epoch 4/300\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 3.0672\n",
      "Epoch 5/300\n",
      "7/7 [==============================] - 2s 299ms/step - loss: 3.0526\n",
      "Epoch 6/300\n",
      "7/7 [==============================] - 2s 299ms/step - loss: 3.0408\n",
      "Epoch 7/300\n",
      "7/7 [==============================] - 2s 358ms/step - loss: 3.0296\n",
      "Epoch 8/300\n",
      "7/7 [==============================] - 3s 461ms/step - loss: 3.0098\n",
      "Epoch 9/300\n",
      "7/7 [==============================] - 3s 413ms/step - loss: 2.9858\n",
      "Epoch 10/300\n",
      "7/7 [==============================] - 3s 379ms/step - loss: 2.9545\n",
      "Epoch 11/300\n",
      "7/7 [==============================] - 2s 317ms/step - loss: 2.9142\n",
      "Epoch 12/300\n",
      "7/7 [==============================] - 2s 330ms/step - loss: 2.8693\n",
      "Epoch 13/300\n",
      "7/7 [==============================] - 2s 303ms/step - loss: 2.8256\n",
      "Epoch 14/300\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 2.7802\n",
      "Epoch 15/300\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 2.7376\n",
      "Epoch 16/300\n",
      "7/7 [==============================] - 2s 318ms/step - loss: 2.7013\n",
      "Epoch 17/300\n",
      "7/7 [==============================] - 2s 330ms/step - loss: 2.6712\n",
      "Epoch 18/300\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 2.6483\n",
      "Epoch 19/300\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 2.6273\n",
      "Epoch 20/300\n",
      "7/7 [==============================] - 3s 440ms/step - loss: 2.6158\n",
      "Epoch 21/300\n",
      "7/7 [==============================] - 2s 354ms/step - loss: 2.5967\n",
      "Epoch 22/300\n",
      "7/7 [==============================] - 3s 371ms/step - loss: 2.5789\n",
      "Epoch 23/300\n",
      "7/7 [==============================] - 3s 366ms/step - loss: 2.5634\n",
      "Epoch 24/300\n",
      "7/7 [==============================] - 3s 374ms/step - loss: 2.5495\n",
      "Epoch 25/300\n",
      "7/7 [==============================] - 3s 364ms/step - loss: 2.5330\n",
      "Epoch 26/300\n",
      "7/7 [==============================] - 2s 359ms/step - loss: 2.5165\n",
      "Epoch 27/300\n",
      "7/7 [==============================] - 2s 315ms/step - loss: 2.5059\n",
      "Epoch 28/300\n",
      "7/7 [==============================] - 2s 311ms/step - loss: 2.4985\n",
      "Epoch 29/300\n",
      "7/7 [==============================] - 2s 328ms/step - loss: 2.4824\n",
      "Epoch 30/300\n",
      "7/7 [==============================] - 2s 333ms/step - loss: 2.4717\n",
      "Epoch 31/300\n",
      "7/7 [==============================] - 3s 390ms/step - loss: 2.4652\n",
      "Epoch 32/300\n",
      "7/7 [==============================] - 2s 318ms/step - loss: 2.4491\n",
      "Epoch 33/300\n",
      "7/7 [==============================] - 2s 331ms/step - loss: 2.4306\n",
      "Epoch 34/300\n",
      "7/7 [==============================] - 2s 333ms/step - loss: 2.4199\n",
      "Epoch 35/300\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 2.4055\n",
      "Epoch 36/300\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 2.3962\n",
      "Epoch 37/300\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 2.3887\n",
      "Epoch 38/300\n",
      "7/7 [==============================] - 2s 308ms/step - loss: 2.3770\n",
      "Epoch 39/300\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 2.3731\n",
      "Epoch 40/300\n",
      "7/7 [==============================] - 2s 315ms/step - loss: 2.3559\n",
      "Epoch 41/300\n",
      "7/7 [==============================] - 2s 313ms/step - loss: 2.3417\n",
      "Epoch 42/300\n",
      "7/7 [==============================] - 3s 363ms/step - loss: 2.3292\n",
      "Epoch 43/300\n",
      "7/7 [==============================] - 2s 311ms/step - loss: 2.3216\n",
      "Epoch 44/300\n",
      "7/7 [==============================] - 2s 312ms/step - loss: 2.3115\n",
      "Epoch 45/300\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 2.3031\n",
      "Epoch 46/300\n",
      "7/7 [==============================] - 2s 314ms/step - loss: 2.2827\n",
      "Epoch 47/300\n",
      "7/7 [==============================] - 2s 315ms/step - loss: 2.2739\n",
      "Epoch 48/300\n",
      "7/7 [==============================] - 2s 313ms/step - loss: 2.2632\n",
      "Epoch 49/300\n",
      "7/7 [==============================] - 2s 316ms/step - loss: 2.2564\n",
      "Epoch 50/300\n",
      "7/7 [==============================] - 2s 328ms/step - loss: 2.2504\n",
      "Epoch 51/300\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 2.2492\n",
      "Epoch 52/300\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 2.2400\n",
      "Epoch 53/300\n",
      "7/7 [==============================] - 2s 314ms/step - loss: 2.2253\n",
      "Epoch 54/300\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 2.2170\n",
      "Epoch 55/300\n",
      "7/7 [==============================] - 2s 311ms/step - loss: 2.2059\n",
      "Epoch 56/300\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 2.1855\n",
      "Epoch 57/300\n",
      "7/7 [==============================] - 2s 317ms/step - loss: 2.1866\n",
      "Epoch 58/300\n",
      "7/7 [==============================] - 2s 318ms/step - loss: 2.1822\n",
      "Epoch 59/300\n",
      "7/7 [==============================] - 2s 335ms/step - loss: 2.1758\n",
      "Epoch 60/300\n",
      "7/7 [==============================] - 2s 355ms/step - loss: 2.1718\n",
      "Epoch 61/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 2.1576\n",
      "Epoch 62/300\n",
      "7/7 [==============================] - 2s 308ms/step - loss: 2.1463\n",
      "Epoch 63/300\n",
      "7/7 [==============================] - 2s 314ms/step - loss: 2.1476\n",
      "Epoch 64/300\n",
      "7/7 [==============================] - 2s 308ms/step - loss: 2.1417\n",
      "Epoch 65/300\n",
      "7/7 [==============================] - 2s 313ms/step - loss: 2.1350\n",
      "Epoch 66/300\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 2.1200\n",
      "Epoch 67/300\n",
      "7/7 [==============================] - 2s 312ms/step - loss: 2.1165\n",
      "Epoch 68/300\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 2.1065\n",
      "Epoch 69/300\n",
      "7/7 [==============================] - 2s 325ms/step - loss: 2.1038\n",
      "Epoch 70/300\n",
      "7/7 [==============================] - 2s 330ms/step - loss: 2.1069\n",
      "Epoch 71/300\n",
      "7/7 [==============================] - 2s 314ms/step - loss: 2.0973\n",
      "Epoch 72/300\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 2.0837\n",
      "Epoch 73/300\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 2.0689\n",
      "Epoch 74/300\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 2.0608\n",
      "Epoch 75/300\n",
      "7/7 [==============================] - 2s 312ms/step - loss: 2.0493\n",
      "Epoch 76/300\n",
      "7/7 [==============================] - 2s 329ms/step - loss: 2.0402\n",
      "Epoch 77/300\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 2.0305\n",
      "Epoch 78/300\n",
      "7/7 [==============================] - 2s 301ms/step - loss: 2.0213\n",
      "Epoch 79/300\n",
      "7/7 [==============================] - 2s 301ms/step - loss: 2.0245\n",
      "Epoch 80/300\n",
      "7/7 [==============================] - 2s 301ms/step - loss: 2.0130\n",
      "Epoch 81/300\n",
      "7/7 [==============================] - 2s 297ms/step - loss: 2.0019\n",
      "Epoch 82/300\n",
      "7/7 [==============================] - 2s 304ms/step - loss: 1.9971\n",
      "Epoch 83/300\n",
      "7/7 [==============================] - 2s 317ms/step - loss: 1.9966\n",
      "Epoch 84/300\n",
      "7/7 [==============================] - 2s 302ms/step - loss: 1.9881\n",
      "Epoch 85/300\n",
      "7/7 [==============================] - 2s 299ms/step - loss: 1.9937\n",
      "Epoch 86/300\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 1.9816\n",
      "Epoch 87/300\n",
      "7/7 [==============================] - 2s 316ms/step - loss: 1.9670\n",
      "Epoch 88/300\n",
      "7/7 [==============================] - 2s 316ms/step - loss: 1.9623\n",
      "Epoch 89/300\n",
      "7/7 [==============================] - 2s 308ms/step - loss: 1.9584\n",
      "Epoch 90/300\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 1.9535\n",
      "Epoch 91/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.9438\n",
      "Epoch 92/300\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 1.9434\n",
      "Epoch 93/300\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 1.9432\n",
      "Epoch 94/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.9378\n",
      "Epoch 95/300\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 1.9313\n",
      "Epoch 96/300\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 1.9351\n",
      "Epoch 97/300\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 1.9250\n",
      "Epoch 98/300\n",
      "7/7 [==============================] - 2s 304ms/step - loss: 1.9142\n",
      "Epoch 99/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.9102\n",
      "Epoch 100/300\n",
      "7/7 [==============================] - 2s 312ms/step - loss: 1.9078\n",
      "Epoch 101/300\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 1.9114\n",
      "Epoch 102/300\n",
      "7/7 [==============================] - 2s 317ms/step - loss: 1.9091\n",
      "Epoch 103/300\n",
      "7/7 [==============================] - 2s 304ms/step - loss: 1.8972\n",
      "Epoch 104/300\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 1.9038\n",
      "Epoch 105/300\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 1.9163\n",
      "Epoch 106/300\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 1.9196\n",
      "Epoch 107/300\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 1.9272\n",
      "Epoch 108/300\n",
      "7/7 [==============================] - 2s 311ms/step - loss: 1.9226\n",
      "Epoch 109/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.9206\n",
      "Epoch 110/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.9119\n",
      "Epoch 111/300\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 1.8939\n",
      "Epoch 112/300\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 1.8785\n",
      "Epoch 113/300\n",
      "7/7 [==============================] - 2s 304ms/step - loss: 1.8706\n",
      "Epoch 114/300\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 1.8536\n",
      "Epoch 115/300\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 1.8517\n",
      "Epoch 116/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.8386\n",
      "Epoch 117/300\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 1.8461\n",
      "Epoch 118/300\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 1.8241\n",
      "Epoch 119/300\n",
      "7/7 [==============================] - 2s 312ms/step - loss: 1.8179\n",
      "Epoch 120/300\n",
      "7/7 [==============================] - 2s 304ms/step - loss: 1.8228\n",
      "Epoch 121/300\n",
      "7/7 [==============================] - 2s 311ms/step - loss: 1.8146\n",
      "Epoch 122/300\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 1.8034\n",
      "Epoch 123/300\n",
      "7/7 [==============================] - 2s 303ms/step - loss: 1.7934\n",
      "Epoch 124/300\n",
      "7/7 [==============================] - 2s 304ms/step - loss: 1.7870\n",
      "Epoch 125/300\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 1.7885\n",
      "Epoch 126/300\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 1.7892\n",
      "Epoch 127/300\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 1.7858\n",
      "Epoch 128/300\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 1.7920\n",
      "Epoch 129/300\n",
      "7/7 [==============================] - 2s 308ms/step - loss: 1.7779\n",
      "Epoch 130/300\n",
      "7/7 [==============================] - 2s 313ms/step - loss: 1.7784\n",
      "Epoch 131/300\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 1.7812\n",
      "Epoch 132/300\n",
      "7/7 [==============================] - 2s 312ms/step - loss: 1.7739\n",
      "Epoch 133/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.7676\n",
      "Epoch 134/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.7572\n",
      "Epoch 135/300\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 1.7460\n",
      "Epoch 136/300\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 1.7432\n",
      "Epoch 137/300\n",
      "7/7 [==============================] - 2s 313ms/step - loss: 1.7398\n",
      "Epoch 138/300\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 1.7372\n",
      "Epoch 139/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.7418\n",
      "Epoch 140/300\n",
      "7/7 [==============================] - 2s 304ms/step - loss: 1.7361\n",
      "Epoch 141/300\n",
      "7/7 [==============================] - 2s 308ms/step - loss: 1.7308\n",
      "Epoch 142/300\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 1.7277\n",
      "Epoch 143/300\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 1.7321\n",
      "Epoch 144/300\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 1.7268\n",
      "Epoch 145/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.7221\n",
      "Epoch 146/300\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 1.7136\n",
      "Epoch 147/300\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 1.7125\n",
      "Epoch 148/300\n",
      "7/7 [==============================] - 2s 308ms/step - loss: 1.7008\n",
      "Epoch 149/300\n",
      "7/7 [==============================] - 2s 308ms/step - loss: 1.6912\n",
      "Epoch 150/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.6860\n",
      "Epoch 151/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.6727\n",
      "Epoch 152/300\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 1.6737\n",
      "Epoch 153/300\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 1.6805\n",
      "Epoch 154/300\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 1.6762\n",
      "Epoch 155/300\n",
      "7/7 [==============================] - 2s 315ms/step - loss: 1.7083\n",
      "Epoch 156/300\n",
      "7/7 [==============================] - 2s 314ms/step - loss: 1.7077\n",
      "Epoch 157/300\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 1.7186\n",
      "Epoch 158/300\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 1.7186\n",
      "Epoch 159/300\n",
      "7/7 [==============================] - 2s 303ms/step - loss: 1.7097\n",
      "Epoch 160/300\n",
      "7/7 [==============================] - 2s 309ms/step - loss: 1.7033\n",
      "Epoch 161/300\n",
      "7/7 [==============================] - 2s 308ms/step - loss: 1.7029\n",
      "Epoch 162/300\n",
      "7/7 [==============================] - 2s 306ms/step - loss: 1.6958\n",
      "Epoch 163/300\n",
      "7/7 [==============================] - 2s 311ms/step - loss: 1.6862\n",
      "Epoch 164/300\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 1.6931\n",
      "Epoch 165/300\n",
      "7/7 [==============================] - 2s 331ms/step - loss: 1.6805\n",
      "Epoch 166/300\n",
      "7/7 [==============================] - 2s 315ms/step - loss: 1.6511\n",
      "Epoch 167/300\n",
      "7/7 [==============================] - 3s 347ms/step - loss: 1.6284\n",
      "Epoch 168/300\n",
      "7/7 [==============================] - 2s 336ms/step - loss: 1.6075\n",
      "Epoch 169/300\n",
      "7/7 [==============================] - 2s 307ms/step - loss: 1.5855\n",
      "Epoch 170/300\n",
      "7/7 [==============================] - 2s 308ms/step - loss: 1.5695\n",
      "Epoch 171/300\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 1.5570\n",
      "Epoch 172/300\n",
      "7/7 [==============================] - 2s 310ms/step - loss: 1.5432\n",
      "Epoch 173/300\n",
      "7/7 [==============================] - 2s 311ms/step - loss: 1.5250\n",
      "Epoch 174/300\n",
      "7/7 [==============================] - 2s 331ms/step - loss: 1.5153\n",
      "Epoch 175/300\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 1.4891WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2100 batches). You may need to use the repeat() function when building your dataset.\n",
      "7/7 [==============================] - 2s 224ms/step - loss: 1.4891\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat, epochs=300, callbacks=[checkpoint_callback], steps_per_epoch=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Bidirectional_LSTM\n",
    "arabic_bidirectional_lstm_check_point_2 = check_point('Arabic_Bidirectional_2')\n",
    "bidirectional_lstm_model = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units= 128, return_sequences=False))\n",
    "Bidirectional_LSTM_2 = nlp_model_char(input_dim = all_chars, output_dim = 100, unit = all_chars, model = bidirectional_lstm_model, input_length=max_length_char)\n",
    "model_compile_char(model =Bidirectional_LSTM_2, optimizer=tf.keras.optimizers.legacy.Adam(), loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "history = model_fit(model=Bidirectional_LSTM_2, Data=Train, Label=Label, epochs=150, early_stop=early_stop_, batch_size=32, checkpoint=arabic_bidirectional_lstm_check_point_2)\n",
    "plot_char_model_change(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-GRU\n",
    "gru_model = tf.keras.layers.GRU(units= 128, return_sequences=False)\n",
    "GRU_2 = nlp_model_char(input_dim = all_chars, output_dim = 100, unit = all_chars, model = gru_model, input_length=max_length_char)\n",
    "model_compile_char(model =GRU_2, optimizer=tf.keras.optimizers.legacy.Adam(), loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "history = model_fit(model=GRU_2, Data=Train, Label=Label, epochs=150, early_stop=early_stop, batch_size=32)\n",
    "plot_char_model_change(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def model_testing_char(text, number_of_chars, char_index, pad_sequences, max_length, index_char, text_to_sequence_char, all_chars, char_sequence_to_text, checkpoint_filepath):\n",
    "    model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "    encoded_text = text_to_sequence_char(char_index, text)\n",
    "    encoded_text = list(list(itertools.chain.from_iterable(encoded_text)))\n",
    "    padded_text = pad_sequences(padding='post', input_sequence=[encoded_text], max_length=max_length)\n",
    "    for _ in range(number_of_chars):\n",
    "        predicted_probs = model.predict(padded_text, verbose=0)\n",
    "        index = np.argmax(predicted_probs)\n",
    "        text += char_sequence_to_text(index_char, [index], all_chars)\n",
    "        padded_text = np.append(padded_text, index)\n",
    "        padded_text = pad_sequences(padding='post', input_sequence=[padded_text], max_length=max_length)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-LSTM\n",
    "model_testing_char(text = 'تناول الخيميائي', number_of_chars = 200, char_index = chars_index, pad_sequences = pad_sequences, checkpoint_filepath= 'Arabic_Bidirectional_2_model_checkpoint.h5', max_length = max_length_char, index_char = index_chars, text_to_sequence_char = text_to_sequence_char, all_chars=all_chars, char_sequence_to_text=char_sequence_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Bidirectional LSTM\n",
    "model_testing_char(text = 'أضاءت أشعة القمر ', number_of_chars = 200, char_index = chars_index, pad_sequences = pad_sequences, model = Bidirectional_LSTM_2, max_length = max_length_char, index_char = index_chars, text_to_sequence_char = text_to_sequence_char, all_chars=all_chars, char_sequence_to_text=char_sequence_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-GRU\n",
    "model_testing_char(text = 'أضاءت أشعة القمر ', number_of_chars = 200, char_index = chars_index, pad_sequences = pad_sequences, model = GRU_2, max_length = max_length_char, index_char = index_chars, text_to_sequence_char = text_to_sequence_char, all_chars=all_chars, char_sequence_to_text=char_sequence_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-Transformers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers Model Name\n",
    "import torch\n",
    "model_name_ = 'gpt2'\n",
    "# Model And Tokenizer For Transformers\n",
    "tokenizer, model = transformes_model(model_name_)\n",
    "# Data Used For Train Transformers\n",
    "train_dataset = load_dataset('Cleaned_Corpus.txt', tokenizer)\n",
    "# Convert Data Into Batches\n",
    "collator = data_collator(tokenizer)\n",
    "# Set Arguments For Train The Transformers\n",
    "train_args = train_arguments(epochs = 1)\n",
    "# Train The Transformers\n",
    "training_(model=model, training_args=train_args, collator = collator, data = train_dataset)\n",
    "# Save The Model\n",
    "save_model_tokenizer(model = model, tokenizer = tokenizer)\n",
    "# Testing The Transformers Model\n",
    "transformer_testing(input_text='إنني خيميائي ', tokenizer=tokenizer, model=model, text_normalization = text_normalization)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
