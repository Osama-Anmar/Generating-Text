{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "tf.random.set_seed(42)\n",
    "from nlp_model_text_preprocessing import index_the_words, text_to_sequence, pad_sequences, one_hot_encoding, index_the_char, text_to_sequence_char, char_sequence_to_text, word_sequence_to_text\n",
    "from english_text_normalization import text_normalization\n",
    "from deep_learning import nlp_model_word, nlp_model_char, model_compile_word, model_compile_char, model_fit, plot_word_model_change, plot_char_model_change\n",
    "from model_testing import model_testing_char, model_testing_word\n",
    "from transformers_models import load_dataset, data_collator, train_arguments, training_, save_model_tokenizer, transformer_testing,transformes_model\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ignore Warnings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3668"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Corpus = open(\"The Alchemist.txt\", encoding='utf-8-sig').read().lower().split(\"\\n\")\n",
    "Corpus = [line.strip() for line in Corpus if line.strip()]\n",
    "Corpus =  list(map(text_normalization, Corpus))\n",
    "len(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Cleaned_Corpus.txt', 'w') as f:\n",
    "    for line in Corpus:\n",
    "        f.write(line + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-Tokens Based On Word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Word To Index, Index To Word, And Find Count Of All Words\n",
    "all_words, words_index, index_to_words = index_the_words(Corpus)\n",
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Input Sequences And Get The Max Length\n",
    "input_sequence_ = []\n",
    "for line in Corpus:\n",
    "    token = text_to_sequence(words_index, line)\n",
    "    for i in range(1, len(token)):\n",
    "        n_grams = token[:i+1]\n",
    "        input_sequence_.append(n_grams)\n",
    "        \n",
    "max_length_word =  max([len(x) for x in input_sequence_])\n",
    "max_length_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding The The Input Sequence To Make All Sequence In Same Length\n",
    "input_sequence_ = pad_sequences(input_sequence=input_sequence_, max_length=max_length_word)\n",
    "input_sequence_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Train And Label Data For Model\n",
    "train, labels = input_sequence_[:,:-1], input_sequence_[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Label Data \n",
    "label = one_hot_encoding(labels, all_words)\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Deep Learning Models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Early Stop Depending On Value Of Loss\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=5,\n",
    "    mode = 'min',\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-LSTM\n",
    "lstm_model = tf.keras.layers.LSTM(units=128, return_sequences=False)\n",
    "LSTM_1 = nlp_model_word(input_dim = all_words, output_dim = 100, input_length = max_length_word, unit = all_words, model = lstm_model)\n",
    "model_compile_word(model =LSTM_1, optimizer=tf.keras.optimizers.legacy.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "history = model_fit(model=LSTM_1, Data=train, Label=label, epochs=150, early_stop=early_stop, batch_size=32)\n",
    "plot_word_model_change(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Bidirectional LSTM\n",
    "bidirectional_lstm_model = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units= 128, return_sequences=False))\n",
    "Bidirectional_LSTM_1 = nlp_model_word(input_dim = all_words, output_dim = 100, input_length = max_length_word, unit = all_words, model = bidirectional_lstm_model)\n",
    "model_compile_word(model =Bidirectional_LSTM_1, optimizer=tf.keras.optimizers.legacy.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "history = model_fit(model=Bidirectional_LSTM_1, Data=train, Label=label, epochs=150, early_stop=early_stop, batch_size=64)\n",
    "plot_word_model_change(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-GRU\n",
    "gru_model = tf.keras.layers.GRU(units= 128, return_sequences=False)\n",
    "GRU_1 = nlp_model_word(input_dim = all_words, output_dim = 100, input_length = max_length_word, unit = all_words, model = gru_model)\n",
    "model_compile_word(model =GRU_1, optimizer=tf.keras.optimizers.legacy.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "history = model_fit(model=GRU_1, Data=train, Label=label, epochs=150, early_stop=early_stop, batch_size=32)\n",
    "plot_word_model_change(history=history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Testing The Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-LSTM\n",
    "model_testing_word(text = ' He decided to wait until the sun had', number_of_words = 25, text_normalization = text_normalization, text_to_sequence = text_to_sequence, words_index = words_index, pad_sequences = pad_sequences, model = LSTM_1, max_length = max_length_word, index_to_words = index_to_words, word_sequence_to_text = word_sequence_to_text, all_words=all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Bidirectional LSTM\n",
    "model_testing_word(text = ' He decided to wait until the sun had', number_of_words = 25, text_normalization = text_normalization, text_to_sequence = text_to_sequence, words_index = words_index, pad_sequences = pad_sequences, model = Bidirectional_LSTM_1, max_length = max_length_word, index_to_words = index_to_words, word_sequence_to_text = word_sequence_to_text, all_words=all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-GRU\n",
    "model_testing_word(text = ' He decided to wait until the sun had', number_of_words = 25, text_normalization = text_normalization, text_to_sequence = text_to_sequence, words_index = words_index, pad_sequences = pad_sequences, model = GRU_1, max_length = max_length_word, index_to_words = index_to_words, word_sequence_to_text = word_sequence_to_text, all_words=all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Save The Models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_1.save('English_LSTM_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bidirectional_LSTM_1.save('English_Bidirectional_LSTM_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_1.save('English_GRU_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-Token Based On Character**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Char To Index, Index To Char, And Find Count Of All Char\n",
    "all_chars, chars_index, index_chars = index_the_char(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find The Max Length\n",
    "max_length_char = max([len(s) for s in Corpus])\n",
    "max_length_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Train And Label Data\n",
    "Text = \" \".join(Corpus)\n",
    "Train = []\n",
    "Label = []\n",
    "for i in range(0, len(Text) - max_length_char):\n",
    "    Train.append(Text[i: i + max_length_char])\n",
    "    Label.append(Text[i + max_length_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Words Into Number\n",
    "sequence_text_char_Train = text_to_sequence_char(chars_index, Train)\n",
    "sequence_text_char_Label = text_to_sequence_char(chars_index, Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding The The Input Sequence To Make All Sequence In Same Length\n",
    "Train = pad_sequences(input_sequence=sequence_text_char_Train, max_length=max_length_char, padding='post')\n",
    "# # Convert The Label Data\n",
    "Label = one_hot_encoding(sequence_text_char_Label, all_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-LSTM\n",
    "lstm_model = tf.keras.layers.LSTM(units= 128, return_sequences=False)\n",
    "LSTM_2 = nlp_model_char(input_dim = all_chars, output_dim = 100, unit = all_chars, model = lstm_model)\n",
    "model_compile_char(model =LSTM_2, optimizer=tf.keras.optimizers.legacy.Adam(), loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "history = model_fit(model=LSTM_2, Data=Train, Label=Label, epochs=150, early_stop=early_stop, batch_size=32)\n",
    "plot_char_model_change(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Bidirectional_LSTM\n",
    "bidirectional_lstm_model = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units= 128, return_sequences=False))\n",
    "Bidirectional_LSTM_2 = nlp_model_char(input_dim = all_chars, output_dim = 100, unit = all_chars, model = bidirectional_lstm_model)\n",
    "model_compile_char(model =Bidirectional_LSTM_2, optimizer=tf.keras.optimizers.legacy.Adam(), loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "history = model_fit(model=Bidirectional_LSTM_2, Data=Train, Label=Label, epochs=150, early_stop=early_stop, batch_size=32)\n",
    "plot_char_model_change(history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-GRU\n",
    "gru_model = tf.keras.layers.GRU(units= 128, return_sequences=False)\n",
    "GRU_2 = nlp_model_char(input_dim = all_chars, output_dim = 100, unit = all_chars, model = gru_model)\n",
    "model_compile_char(model =GRU_2, optimizer=tf.keras.optimizers.legacy.Adam(), loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "history = model_fit(model=GRU_2, Data=Train, Label=Label, epochs=150, early_stop=early_stop, batch_size=32)\n",
    "plot_char_model_change(history=history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-LSTM\n",
    "model_testing_char(text = 'He decided', number_of_chars = 100, char_index = chars_index, pad_sequences = pad_sequences, model = LSTM_2, max_length = max_length_char, index_char = index_chars, text_to_sequence_char = text_to_sequence_char, all_chars=all_chars, char_sequence_to_text=char_sequence_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Bidirectional LSTM\n",
    "model_testing_char(text = 'He decided', number_of_chars = 100, char_index = chars_index, pad_sequences = pad_sequences, model = Bidirectional_LSTM_2, max_length = max_length_char, index_char = index_chars, text_to_sequence_char = text_to_sequence_char, all_chars=all_chars, char_sequence_to_text=char_sequence_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-GRU\n",
    "model_testing_char(text = 'He decided', number_of_chars = 100, char_index = chars_index, pad_sequences = pad_sequences, model = GRU_2, max_length = max_length_char, index_char = index_chars, text_to_sequence_char = text_to_sequence_char, all_chars=all_chars, char_sequence_to_text=char_sequence_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_2.save('English_LSTM_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bidirectional_LSTM_2.save('English_Bidirectional_LSTM_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_2.save('English_GRU_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-Transformers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers Model Name\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "model_name_ = 'gpt2'\n",
    "# Model And Tokenizer For Transformers\n",
    "tokenizer, model = transformes_model(model_name_)\n",
    "model.to(device)\n",
    "# Data Used For Train Transformers\n",
    "train_dataset = load_dataset('Cleaned_Corpus.txt', tokenizer)\n",
    "# Convert Data Into Batches\n",
    "collator = data_collator(tokenizer)\n",
    "# Set Arguments For Train The Transformers\n",
    "train_args = train_arguments(epochs = 1)\n",
    "# Train The Transformers\n",
    "training_(model=model.to(device), training_args=train_args, collator = collator, data = train_dataset)\n",
    "# Save The Model\n",
    "save_model_tokenizer(model = model, tokenizer = tokenizer)\n",
    "# Testing The Transformers Model\n",
    "transformer_testing(input_text='He decided to wait until the sun ', tokenizer=tokenizer, model=model, text_normalization = text_normalization)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
